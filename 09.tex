\chapter{}

\topic{Ore extensions}

Let $R$ be an algebra with one (over a field $K$). 
We will consider polynomials over $R$. What does it mean? $R[X]$ will 
be the free (left) $R$-module of the elements of the form
\[
f=a_nX^n+a_{n-1}X^{n-1}+\cdots+a_1X+a_0X^0,
\]
where $a_n,a_{n-1},\dots,a_1,a_0\in R$. By convention, we write $X^0=1$. 
If $a_n\ne 0$, the element
$f$ is said to be of degree $n$, that is 
$\deg(f)=n$. By convention,
$\deg(0)=-\infty$. 

\begin{exercise}
    Compute the quotient of $\H[X]$ by the ideal generated by $X-i$. 
\end{exercise}

The previous exercise shows that non-commutative polynomial rings are weird. Note that $i$ is not a central 
element of the quaternion algebra $\H$, so we are not allowed to use standard evaluation techniques to compute
the quotient. Furthermore, one can prove that the ideal $I$ generated by $X-i$ is equal to $\H[X]$. 

\begin{definition}
\index{Derivation}
    Let $R$ be an algebra and $\alpha\in\End(R)$. A linear transformation $\alpha\colon R\to R$ is a \textbf{derivation}
    with respect to $\alpha$ (or an 
    $\alpha$-derivation) if 
    \[
    \delta(ab)=\alpha(a)\delta(b)+\delta(a)b
    \]
    for all $a,b\in R$. 
\end{definition}

What can we say about algebra structures on $R[X]$ compatible with that of $R$ and 
the degree function? 

\begin{theorem}
    Let $R$ be an algebra and $R[X]$ be an algebra such that 
    the canonical embedding $R\to R[X]$ is an algebra homomorphism and $\deg(fg)=\deg(f)+\deg(g)$ 
    for all $f,g\in R[X]$. Then $R$ has no non-zero zero divisors and there exists a unique injective algebra homomorphism 
    $\alpha\colon R\to R$ and a unique $\alpha$-derivation such that 
    \[
    Xa=\alpha(a)X+\delta(a)
    \]
    for all $a\in R$.    
\end{theorem}

\begin{proof}
    Let $a,b\in R\setminus\{0\}$. Since $\deg(ab)=\deg(a)+\deg(b)=0$, 
    $ab\ne 0$. Thus $R$ has no non-zero zero divisors. Since 
    \[
    \deg(Xa)=\deg(X)+\deg(a)=1,
    \]
    there exist $\alpha(a)\in R\setminus\{0\}$ and $\delta(a)\in R$ such that 
    $Xa=\alpha(a)X+\delta(a)$. Thus we have well-defined maps
    $\alpha\colon R\to R$ and $\delta\colon R\to R$. Note that 
    \[
    X(a+\lambda b)=Xa+\lambda Xb
    \]
    for all $a,b\in R$ and $\lambda\in K$. Thus $\alpha$ and $\delta$ are both 
    linear transformations. Since 
    $(Xa)b=X(ab)$ in $R[X]$, the formula 
    \[
    \alpha(a)\alpha(b)X+\alpha(a)\delta(b)+\delta(a)b=\alpha(ab)X+\delta(ab)
    \]
    implies that $\alpha(ab)=\alpha(a)\alpha(b)$ and $\delta(ab)=\alpha(a)\delta(b)+\delta(a)b$ 
    hold for all $a,b\in R$. Moreover, $X1=X$ implies that $\alpha(1)=1$ and $\delta(1)=0$. 
\end{proof}

\begin{theorem}
\label{thm:Ore}
    Let $R$ be an algebra with no non-zero zero divisors, 
    $\alpha\colon R\to R$ an injective algebra homomorphism and $\delta$ an $\alpha$-derivation. 
    There exists a unique algebra structure on $R[X]$ such that 
    the canonical inclusion $R\to R[X]$ is an algebra homomorphism and 
    \[
    Xa=\alpha(a)X+\delta(a)
    \]
    for all $a\in R$. 
\end{theorem}

\begin{proof}
    Let $M$ be the algebra of infinite matrices $(f_{ij})_{i,j\geq1}$ with coefficients 
    in $\End(R)$, where both each file and each row have only finitely many non-zero entries. The neutral 
    element is the diagonal matrix 
    \[
    I=\begin{pmatrix}
    1&0&0&0&\cdots\\
    0&1&0&0&\cdots \\
    0&0&1&0&\cdots \\
    \vdots&\vdots&\vdots&\vdots&\ddots\\
    \end{pmatrix}.
    \]
    For each $a\in R$, let $L_a\colon R\to R$, $x\mapsto ax$. Then
    \begin{equation}
    \label{eq:alpha_delta}
    \alpha L_a=L_{\alpha(a)}\alpha,\quad  
    \delta L_a=L_{\alpha(a)}\delta+L_{\delta(a)}
    \end{equation}
    for all $a\in R$. Let 
    \[
    T=\begin{pmatrix}
    \delta&0&0&0&\cdots\\
    \alpha&\delta&0&0&\cdots \\
    0&\alpha&\delta&0&\cdots \\
    \vdots&\vdots&\vdots&\vdots&\ddots\\
    \end{pmatrix}\in M
    \]
    and consider the map 
    \[ 
    \Phi\colon R[X]\to M,\quad 
    \sum a_iX^i\mapsto \sum (L_{a_i}I)T^i.
    \]
    We claim that $\Psi$ is injective. First note that, if $\{e_1,e_2,\dots\}$ is the standard basis 
    of $\End(R)$, then $Te_i=e_{i+1}$ for all $i\geq 1$. If $\Phi(\sum a_iX^i)=0$, then
    \[
    0=\Phi\left(\sum a_iX^i\right)e_1=\sum (L_{a_i}I)T^ie_i=\sum (L_{a_i}I)e_{i+1}
    \]
    and hence $a_i=0$ for all $i$. 

    From \eqref{eq:alpha_delta} one gets that 
    \[
    T(L_aI)=(L_{\alpha(a)}I)T+L_{\delta(a)}I
    \]
    holds for all $a\in R$. 

    Let $S$ be the subalgebra of $M$ generated by $T$ and 
    $\{L_aI:a\in R\}$. Then $S=\Phi(R[X])$ and therefore $\Psi\colon R[X]\to S$ is an isomorphism. 
\end{proof}

\begin{definition}
    \index{Ore extension}
    The algebra constructed in Theorem \ref{thm:Ore} is called \textbf{Ore extension} (with respect to $\alpha$ and $\delta$) 
    and will be denoted
    by $R[X,\alpha,\delta]$. 
\end{definition}

The following result is an immediate consequence of Theorem \ref{thm:Ore}.

\begin{corollary}
    Let $R$ be an algebra with no non-zero divisors, $\alpha\in\End(R)$ and 
    $\delta$ be an $\alpha$-derivation. Then $R[X,\alpha,\delta]$ has no non-zero divisors 
    and is a free left $R$-module with basis 
    $\{X^i:i\geq 0\}$. 
\end{corollary}

We can go further in the case where $\alpha$ is invertible. 

\begin{corollary}
    Let $R$ be an algebra with no non-zero divisors, $\alpha\in\Aut(R)$ and 
    $\delta$ be an $\alpha$-derivation. Then $R[X,\alpha,\delta]$ has no non-zero divisors 
    and is a free right $R$-module with basis 
    $\{X^i:i\geq 0\}$. 
\end{corollary}

\begin{proof}
    Let $f\in R[X,\alpha,\delta]$. We claim that there are finitely many 
    elements $a_0,\dots,a_n\in R$ 
    such that $f=\sum X^ia_i$. We proceed by induction on $n$. The case where $n=0$ is trivial. So assume
    the result holds for every $k<n$. Since  
    \[
    aX^n=X^n\alpha^{-n}(a)+g(n)
    \]
    where $g(n)$ is a sum of terms of degree $<n$, 
    the inductive hypothesis implies the claim. 

    Let us prove now that $\{X^i:i\geq 0\}$ is linearly independent. If not, say 
    if \[
    X^na_n+\cdots+Xa_1+a_0=0
    \]
    with $a_n\ne 0$, then 
    $0=\alpha^n(a_n)X^n+\cdots$. 
    By the previous corollary, $\alpha^n(a_n)=0$ and hence $a_n=0$, a contradiction. 
\end{proof}

\begin{example}
    If $\alpha=\id$ and $\delta=0$, then $R[X,\alpha,\delta]\simeq R[X]$. 
\end{example}

\begin{example}
\index{Weyl algebra}
    If $R=K[X]$, $\alpha=\id$ and $\delta=\frac{d}{dX}$, then 
    $R[X,\alpha,\delta]$ is known as the \textbf{Weyl algebra}. 
\end{example}

A ring $R$ is said to be \textbf{left noetherian} if 
every left ideal of $R$ is finitely generated. Routine calculations show
that $R$ is left noetherian if and only if
every sequence 
$I_1\subseteq I_2\subseteq\cdots$
of left ideals of $R$ stabilizes, that is that there exists $n$ such that 
$I_n=I_{n+k}$ for all $k\geq0$. One proves that epimorphic images of left noetherian rings
are left noetherian. 

\begin{theorem}
    Let $R$ be an algebra, $\alpha\in\Aut(R)$ and $\delta$ be an $\alpha$-derivation. If $R$ 
    is left noetherian, then $R[X,\alpha,\delta]$ is left noethetian. 
\end{theorem}

\begin{proof}
    Let $I$ be a left ideal of $R[X,\alpha,\delta]$. For $d\geq0$, let 
    \[
    I_d=\{0\}\cup...
    \]
\end{proof}

\topic{Non-commutative localization}

\begin{definition}
\index{Left quotient ring}
\index{Right quotient ring}
    Let $R$ be a unitary ring and $S$ be a non-empty subset of $R$ such that 
    $1\in S$ and $0\not\in S$. We say that a ring $Q$ is a 
    \textbf{left quotient} of $R$ with respect to $S$ if there is 
    a ring homomorphism $\varphi\colon R\to Q$ such that
    \begin{enumerate}
        \item $\varphi(s)$ is a unit for all $s\in S$, 
        \item every element of $Q$ is of the form $\varphi(s)^{-1}\varphi(r)$ 
        for some $r\in R$ and $s\in S$, and 
        \item $\ker\varphi=\{r\in R:sr=0\text{ for some $s\in S$}\}$. 
    \end{enumerate}
\end{definition}

In the same way, one defines \textbf{right quotients}. One needs to use 
the product $\varphi(r)\varphi(s)^{-1}$ in the second condition and 
replace the third condition 
by 
\[
\ker\varphi=\{r\in R:rs=0\text{ for some $s\in S$}\}. 
\]  

\begin{exercise}
    Let $R$ be a ring and $S$ be a non-empty subset of $R$ such that 
    $1\in S$ and $0\not\in S$. A left quotient ring of $R$ with respect to $S$, if exists, is unique. 
\end{exercise}

%\begin{proof}
%    Let $f\colon R\to R_1$ be a ring homomorphism. If $Q$ 
%    is a left quotient of $R$ with respect to a set $S$ and 
%    $f(s)$ is a unit of $R_1$ for every $s\in S$, then there exists
%    a unique $g\colon Q\to R_1$ that extends $f$. 
%\end{proof}

The left (resp. right) quotient ring of $S$ 
with respect to $S$ will be denoted by $S^{-1}R$ (resp. $RS^{-1}$). 

\begin{exercise}
    Prove that if $R$ admits a left quotient ring $Q$ with respect to $S$ and a right quotient ring $Q_1$ with
    respect to $S$, then $Q\simeq Q_1$.  
\end{exercise}

\begin{definition}
    Let $R$ be a ring and 
    $S$ be a non-empty multiplicatively closed subset of $R$ such that $0\not\in 0$ and 
    $1\in S$. We say that $S$ is a \textbf{left denominator set} if 
    the following conditions hold:
    \begin{enumerate}
        \item $S$ is left Ore.
        \item For $r\in R$ and $s\in S$ with $rs=0$ there exists $s_1\in S$ such that $s_1r=0$. 
    \end{enumerate}
\end{definition}

\begin{theorem}
    Let $R$ be a ring and $S$ be a non-empty multiplicatively closed subset of $R$ such that $0\not\in 0$ and 
    $1\in S$. Then there exists a left quotient ring of $R$ with respect to $S$
    if and only if $S$ is a left denominator set. 
\end{theorem}

\begin{proof}
    
\end{proof}

\topic{Golod--Shafarevich theorem}

\topic{The exterior algebra}

\begin{definition}
\index{Exterior algebra}
    Let $K$ be a field and $n\geq1$. The $n$-th 
    \textbf{exterior algebra} 
    $\Lambda_n(K)$ is the $K$-algebra
    with generators $x_1,\dots,x_n$ and relations 
    \[
    x_ix_j+x_jx_i=0
    \]
    for all $i,j\in\{1,\dots,n\}$. 
\end{definition}

If $K$ has characteristic two, then $\Lambda_n(K)$ is isomorphic to 
the polynomial algebra $K[X_1,\dots,X_n]$ in $n$ commutative variables $X_1,\dots,X_n$. If the characteristic 
of $K$ is different from two, then the relations 
$x_i^2=0$ hold in $\Lambda_n(K)$ 
for all $i\in\{1,\dots,n\}$. 

% \begin{example}
%     Let us examine the case $n=2$. Every element of $\Lambda_2(K)$ can be written uniquely 
%     as \[
%     \lambda+\lambda_1 x_1+\lambda_2x_2+\mu x_1x_2.
%     \]
% \end{example}

\begin{theorem}
    Let $K$ be a field of characteristic $\ne2$. Then 
    the monomials of the form 
    \[
    x_{i_1}\cdots x_{i_k}
    \]
    with $1\leq k\leq n$ and $i_1<\cdots<i_k\leq n$, together with $1$, form 
    a basis of $\Lambda_n(K)$. In particular, $\dim\Lambda_n(K)=2^n$. 
\end{theorem}

\begin{proof}
    Since $x_i^2=0$ for all $i$ and $x_ix_j=-x_jx_i$ for all $i,j$, it follows that 
    every monomial is either one or of the form $x_{i_1}x_{i_2}\cdots x_{i_k}$ 
    for some $k\leq n$. In particular 
    \[
    \dim\Lambda_n(K)=1+\sum_{k=1}^n\binom{n}{k}=2^n.
    \]
\end{proof}

\begin{exercise}
    Prove that $x_i\Lambda_n(K)x_i=\{0\}$ for all $i\in\{1,\dots,n\}$. 
\end{exercise}

% x_i(x_{i_1}x_{i_2}\cdots x_{i_k})x_i=\pm x_i^2x_{i_1}x_{i_2}\cdots x_{i_k}=0$. 

\begin{exercise}
    Let $z_1,\dots,z_{n+1}\in\Lambda_n(K)$ be elements with zero constant term. Prove that 
    $z_1z_2\cdots z_{n+1}=0$. 
\end{exercise}

% Every monomial appearing in $z$ is a product of at least $n+1$ elements of $\{x_1,\dots,x_n\}$. Thus it is zero. 

\begin{exercise}
    Prove the following statements:
    \begin{enumerate}
        \item Every element of $\Lambda_n(K)$ is either a unit or a zero divisor. 
        \item An element of $\Lambda_n(K)$ is a zero divisor if and only if its constant term is zero.
        \item The non-units of of $\Lambda_n(K)$ form a nilpotent ideal. 
    \end{enumerate}    
\end{exercise}

% Since $\Lambda_n(K)$ is finite-dimensional, $\Lambda_n(K)$ is algebraic. 

\topic{Quaternion algebras}

\begin{definition}
\index{Quaternion algebra}
    Let $K$ be a field and $a,b\in K\setminus\{0\}$. The \textbf{quaternion algebra}
    with respect to $a,b$ (over $K$) is the $K$-algebra
    $(a,b)_K$ given by generators $i,j$ and relations
    \[
    i^2=a,\quad 
    j^2=b,\quad 
    ij=-ji.
    \] 
\end{definition}

The $K$-algebra $(a,b)_K$ is the quotient of the free algebra $K\langle x,y\rangle$ 
by the ideal generated by $x^2-a$, $y^2-b$ and $xy+yx$. Write
$i=x+I$, $j=y+I$ and $k=ij$. Then every element $\alpha\in(a,b)_K$ admits a unique
representation of the form
\[
\alpha=\lambda_1+\lambda_2x+\lambda_3y+\lambda_4xy+I=\lambda_1+\lambda_2i+\lambda_3j+\lambda_4k
\]
for $\lambda_1,\lambda_2,\lambda_3,\lambda_4\in K$. 
In particular, $\{1,i,j,k\}$ is a basis of $(a,b)_K$. 

\begin{example}
The real algebra $(-1,-1)_{\R}$ is the classical 
4-dimensional real 
quaternion algebra with basis $\{1,i,j,k\}$ with 
$i^2=j^2=k^2=ijk=-1$. 
\end{example}

\begin{exercise}
    If $\alpha=\lambda_1+\lambda_2i+\lambda_3j+\lambda_4k\in (a,b)_K$, let 
    \[
    \overline{\alpha}=\lambda_1-\lambda_2i-\lambda_3j-\lambda_4k. 
    \]
    Prove that the map 
    \[
    N\colon (a,b)_K\to K,
    \quad 
    \alpha\mapsto \alpha\overline{\alpha}=\lambda_1-a\lambda_2^2-b\lambda_3^2+ab\lambda_4^2,
    \]
    is a multiplicative function, that is $N(\alpha\beta)=N(\alpha)N(\beta)$ for all $\alpha,\beta\in (a,b)_K$. 
\end{exercise}

\begin{exercise}
    Prove that a quaternion algebra 
    $(a,b)_K$ is a division algebra if and only if $N(\alpha)\ne 0$ for all $\alpha\ne0$. 
\end{exercise}

Over fields of characteristic two, quaternion algebras are commutative. 

\begin{exercise}
    Let $K$ be a field of characteristic $\ne2$. 
    Let $A=(a,b)_K$ be a quaternion algebra. 
    Prove that $Z(A)=K$. 
\end{exercise}

The previous exercise shows that, in particular, $(-1,-1)_{\R}$ is a division algebra. 
What happens with the quaternion algebra $(1,1)_{\R}$?

\begin{theorem}
    Let $K$ be a field of characteristic $\ne2$. Then the quaternion algebra
    $(a,b)_K$ is simple. 
\end{theorem}

\begin{proof}
    Let $A=(a,b)_K$ and $I$ be a non-zero ideal of $A$. Let $0\ne\alpha\in I$. 
    Write 
    \[
    \alpha=\lambda_1+\lambda_2i+\lambda_3j+\lambda_4k.
    \]
    Without loss of generality, 
    we may assume that $\lambda_1\ne 0$ (if not, replace $\alpha$ by $\alpha i$, $\alpha j$ or 
    $\alpha k$). Since 
    \[
    2a(\lambda_1+\lambda_2 i)=i\alpha i+a\alpha\in I
    \]
    and $2a\ne 0$, $\beta=\lambda_1+\lambda_2 i\in I$. Moreover, 
    $2b\lambda_1=j\beta j+b\beta\in I$ and hence $\lambda_1\in I$. This implies
    that $I=A$, as $1=\lambda_1^{-1}\lambda_1\in I$. 
\end{proof}

\begin{corollary}
    Let $K$ be a field of characteristic $\ne2$ and $A=(a,b)_K$. 
    Then either $A$ is a division algebra or $A\simeq M_2(K)$. 
\end{corollary}

\begin{proof}
    We know that $\dim_KA=4$. The previous theorem shows that 
    $A$ is simple. By Artin--Wedderburn, $A\simeq M_n(D)$ for some 
    division algebra $D$. Since 
    \[
    4=\dim_KA=\dim_KM_n(D)=n^2\dim_KD,
    \]
    either $n=1$ and $\dim_KD=4$ or $n=2$ and $\dim_KD=1$. This means
    that either $A\simeq D$ is a division algebra or $A\simeq M_2(K)$. 
\end{proof}

\topic{Clifford algebras}

\begin{definition}
\index{Clifford algebra}
    Let $K$ be a field and $a_1,\dots,a_n\in K\setminus\{0\}$. 
    The \textbf{Clifford algebra} $C_K(a_1,\dots,a_n)$ 
    is the $K$-algebra with generators $x_1,\dots,x_n$ and relations
    $x_ix_j=-x_jx_i$ for all $i\ne j$ and $x_i^2=a_i$ for all $i$. 
\end{definition}

Note that $\Lambda_n(K)=C_K(0,\dots,0)$ and 
$(a,b)_K=C_K(a,b)$. 

\begin{theorem}
    Let $K$ be a field of characteristic $\ne2$ and 
    $A=C_K(a_1,\dots,a_n)$ be a Clifford algebra. 
    Then $\dim_KA=2^n$. Moreover, $A$ is semisimple if and only if $a_i\ne0$ for all $i$. 
\end{theorem}

\begin{proof}
    We first prove that the set
    \[
    \{1\}\cup \{x_{i_1}\cdots x_{i_m}:m\neq n,1\leq i_1<\cdots<i_m\leq n\}
    \]
    is a basis of $A$. In particular, $\dim_KA=2^n$.

    We claim that $J(A)=\{0\}$ if and only if... 
\end{proof}

\topic{Amitsur--Levitski theorem}

In this section, we will prove a beautiful theorem 
on matrices. The first proof was given by Amitsur and Levitski. 
There is a beautiful graph theoretical proof; see...
The proof we present here goes back to Rosset. We first present 
an auxiliary result. 

\begin{exercise}
    \label{xca:traces}
    Let $A\in M_n(\C)$. If $\trace(A^k)=0$ for all $k\geq n$, then 
    $A^n=0$. 
\end{exercise}

Now we are ready to prove the theorem. 

\begin{theorem}[Amitsur--Levitski]
    \label{thm:AmitsurLevitski}
    \index{Amitsur-Levitski theorem}
    Let $A_1,\dots,A_{2n}\in M_n(\C)$. Then 
    \[
    \sum_{\sigma\in\Sym_{2n}}\operatorname{sign}(\sigma)A_{\sigma(1)}\cdots A_{\sigma(2n)}=0.
    \]
\end{theorem}

\begin{proof}
    For $k\geq1$, write 
    \[
    s_k(A_1,\dots,A_k)=\sum_{\sigma\in\Sym_k}\operatorname{sign}(\sigma)A_{\sigma(1)}\cdots A_{\sigma(k)}=0.
    \]
    Let $G$ be the Grassman algebra, that is 
    the algebra given by generators $x_1,x_2\dots$ and relations 
    $x_i^2=0$ for all $i$ and 
    $x_ix_j+x_jx_i=0$ for all $i,j$. Every element is a finite sum 
    of the form
    \[
    \lambda+\sum \lambda_{i_1i_2\cdots i_k}x_{i_1}x_{i_2}\cdots x_{i_k}.
    \]
    
    For $B=(b_{ij})\in M_n(\C)$ and $x\in G$, let $Bx\in M_n(G)$ be given by
    $(Bx)_{ij}=(b_{ij}x)$. Note that the product of these matrices is given by
    $(Bx)(Cy)=(BC)(xy)$. Let 
    \[
    A=\sum_{j=1}^{2n}A_jx_j\in M_n(G).
    \]
    %Since $x_jGx_j=\{0\}$ for all $j$,  
    We claim that 
    \[
    A^{k}=\sum_{i_1<\cdots<i_k}s_k(A_{i_1},\dots,A_{i_k})x_{i_1}\cdots x_{i_k}.
    \]
    In particular, $A^{2n}=s_{2n}(A)x_1\cdots x_{2n}$. 

    We need to prove that $A^{2n}=(A^2)^k=0$. By Exercise \ref{xca:traces}, 
    it is enough to prove that $(A^2)^k=0$ for all $k\leq n$. Write 
    $A^{2k-1}=\sum_j B_jy_j$ for $y_1,y_2...\in G_1$. Then 
    \begin{align*}
        \trace(A^{2k})&=\trace(AA^{2k-1})
        =\trace\left(\sum_{i,j}A_iB_jx_iy_j\right)
        =\sum_{i,j}\trace(A_iB_j)x_iy_j.
    \end{align*}
    On the other hand, 
    \begin{align*}
        \trace(A^{2k})&=\trace(A^{2k-1}A)
        =\trace\left(\sum_{i,j}B_jA_iy_jx_i\right)
        =\sum_{i,j}\trace(B_jA_i)y_jx_i.
    \end{align*}
    Since $\trace(B_jA_i)=\trace(A_iB_j)$ and $y_jx_i=-x_iy_j$, it follows that 
    $\trace(A^{2k})=0$. 
\end{proof}